<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>model-compression on Girish Varma</title>
    <link>https://girishvarma.in/projects/model-compression/</link>
    <description>Recent content in model-compression on Girish Varma</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://girishvarma.in/projects/model-compression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Expander Networks: Efficient Deep Networks from Graph Theory</title>
      <link>https://girishvarma.in/publication/xnet/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/xnet/</guid>
      <description>Efficient CNN designs like ResNets and DenseNet were proposed to improve accuracy vs efficiency trade-offs. They essentially increased the connectivity, allowing efficient information flow across layers. Inspired by these techniques, we propose to model connections between filters of a CNN using graphs which are simultaneously sparse and well connected. Sparsity results in efficiency while well connectedness can preserve the expressive power of the CNNs. We use a well-studied class of graphs from theoretical computer science that satisfies these properties known as Expander graphs.</description>
    </item>
    
    <item>
      <title>Efficient Semantic Segmentation using Gradual Grouping</title>
      <link>https://girishvarma.in/publication/grad-group/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/grad-group/</guid>
      <description>Deep CNNs for semantic segmentation have high memory and run time requirements. Various approaches have been proposed to make CNNs efficient like grouped, shuffled, depth-wise separable convolutions. We study the effectiveness of these techniques on a real-time semantic segmentation architecture like ERFNet for improving runtime by over 5X. We apply these techniques to CNN layers partially or fully and evaluate the testing accuracies on Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs, giving accuracy scores for models that can run under specified runtime budgets.</description>
    </item>
    
    <item>
      <title>Class2Str: End to End Latent Hierarchy Learning</title>
      <link>https://girishvarma.in/publication/class2str/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/class2str/</guid>
      <description>Deep neural networks for image classification typically consists of a convolutional feature extractor followed by a fully connected classifier network. The predicted and the ground truth labels are represented as one hot vectors. Such a representation assumes that all classes are equally dissimilar. However, classes have visual similarities and often form a hierarchy. Learning this latent hierarchy explicitly in the architecture could provide invaluable insights. We propose an alternate architecture to the classifier network called the Latent Hierarchy (LH) Classifier and an end to end learned Class2Str mapping which discovers a latent hierarchy of the classes.</description>
    </item>
    
    <item>
      <title>Compressing Models for Recognizing Places</title>
      <link>https://girishvarma.in/publication/compression-place/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://girishvarma.in/publication/compression-place/</guid>
      <description>Visual place recognition on low memory devices such as mobile phones and robotics systems is a challenging problem. The state of the art models for this task uses deep learning architectures having close to 100 million parameters which takes over 400MB of memory. This makes these models infeasible to be deployed in low memory devices and gives rise to the need of compressing them. Hence we study the effectiveness of model compression techniques like trained quantization and pruning for reducing the number of parameters on one of the best performing image retrieval models called NetVLAD.</description>
    </item>
    
  </channel>
</rss>